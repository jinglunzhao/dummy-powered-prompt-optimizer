---
alwaysApply: true
---

# AI Dummy Social Skills Testing System - Project Overview

This is an educational chatbot system designed to test and improve social skills training prompts using AI dummy personalities.

## Core Concept

AI "dummies" (simulated college students with social anxiety) interact with AI assistants using different prompts. The system measures which prompts lead to the best improvements in social skills.

## Key Components

### Data Models
- [models.py](mdc:models.py) - All Pydantic data models
  - AIDummy, PersonalityProfile, SocialAnxietyProfile
  - ConversationBasedProfile, EvolutionStage, PersonalityEvolution
  - Assessment, Conversation, TestSession

### Assessment System
- [assessment_system_llm_based.py](mdc:assessment_system_llm_based.py) - LLM-based personality assessment
- [prompts/assessment_prompts.yaml](mdc:prompts/assessment_prompts.yaml) - Assessment prompt templates

### Evolution System
- [personality_materializer.py](mdc:personality_materializer.py) - Materializes abstract traits into concrete examples
- Integrated into models.py as ConversationBasedProfile

### Experiments
- [conversation_length_experiment_with_evolution.py](mdc:conversation_length_experiment_with_evolution.py) - Experiment runner with personality evolution
- [test_gepa_system.py](mdc:test_gepa_system.py) - Main experiment runner that optimizes AI assistant's system prompt with GEPA method

### Prompt Templates (Static)
- [prompts/default_prompts.yaml](mdc:prompts/default_prompts.yaml) - **Baseline/starting prompts** (not all tested prompts!)
- [prompts/conversation_prompts.yaml](mdc:prompts/conversation_prompts.yaml) - Conversation mechanics (appended to all prompts)
- [prompts/assessment_prompts.yaml](mdc:prompts/assessment_prompts.yaml) - Assessment prompt templates
- [prompts/materializer_prompts.yaml](mdc:prompts/materializer_prompts.yaml) - Trait materialization prompts
- [prompts/optimizer_prompts.yaml](mdc:prompts/optimizer_prompts.yaml) - Optimizer prompts

### Experiment Results & Generated Prompts (Dynamic)
- **`data/experiments/gepa_optimization_exp_*.json`** - GEPA-generated prompts and results
  - Contains `all_prompts` (every generated prompt) and `pareto_frontier` (best prompts)
- **`data/experiments/continuous_conversation_with_evolution_exp_*.json`** - Conversation length experiment results
  - Tests one prompt at various conversation lengths with personality evolution

## Important Conventions

### Prompt Storage Architecture

**Static Baseline Prompts** (in YAML files):
- `default_prompts.yaml` contains only 2 baseline prompts used as starting points
- These are NOT the optimized/tested prompts - just the foundation
- `conversation_prompts.yaml` contains technical conversation mechanics (always appended)

**Generated/Tested Prompts** (in experiment JSON files):
- GEPA optimization creates hundreds of evolved prompts stored in experiment results
- Each `gepa_optimization_exp_*.json` contains:
  - `all_prompts[]` - Every prompt generated during evolution
  - `pareto_frontier[]` - Best performing prompts
- Conversation experiments test one prompt and store results, not new prompts

### Personality Evolution
- Controlled by `Config.ENABLE_PERSONALITY_EVOLUTION` flag
- Dummies have **original** (static) and **current** (dynamic) traits
- Reset to original between prompt tests for fair comparison
- Evolution tracks trait materialization (abstract → concrete)

### Assessment Flow
1. Pre-assessment (baseline)
2. Multiple conversations with different prompts
3. Post-assessment (evolved state)
4. Calculate improvement scores

## File Organization

```
/
├── models.py                    # Core data models
├── assessment_system_llm_based.py
├── personality_materializer.py
├── conversation_simulator.py
├── prompt_optimizer.py          # GEPA prompt optimization engine
├── conversation_length_experiment_with_evolution.py
├── test_gepa_system.py          # Main GEPA experiment runner
├── web_interface.py             # Flask web dashboard
├── prompts/                     # Static prompt templates only
│   ├── default_prompts.yaml     # Baseline starting prompts (2 prompts)
│   ├── conversation_prompts.yaml # Conversation mechanics (appended)
│   ├── assessment_prompts.yaml
│   ├── materializer_prompts.yaml
│   └── optimizer_prompts.yaml
├── data/
│   ├── ai_dummies.json          # Generated dummy personalities
│   ├── experiments/             # **Where generated prompts are stored**
│   │   ├── gepa_optimization_exp_*.json       # GEPA results with all_prompts
│   │   └── continuous_conversation_*.json     # Conversation experiment results
│   ├── conversations/           # Individual conversation logs
│   └── personality_evolution/   # Evolution tracking data
└── templates/                   # HTML templates for web interface
```

## Testing Philosophy

### Two Types of Experiments

**GEPA Optimization** ([test_gepa_system.py](mdc:test_gepa_system.py)):
1. Starts with simple baseline prompt from `default_prompts.yaml`
2. Evolves prompts through generations using reflection and mutation
3. Tests each generated prompt with same AI dummies
4. Stores ALL generated prompts in `data/experiments/gepa_optimization_exp_*.json`
5. Goal: **Discover optimal coaching prompt**

**Conversation Length Experiments** ([conversation_length_experiment_with_evolution.py](mdc:conversation_length_experiment_with_evolution.py)):
1. Takes ONE prompt (from `default_prompts.yaml` or command line)
2. Tests conversation at different lengths (milestones: 5, 10, 15, 20 turns)
3. Tracks personality evolution during conversations
4. Stores results in `data/experiments/continuous_conversation_with_evolution_exp_*.json`
5. Goal: **Measure impact of conversation length**

### How Prompts Work Together

When a conversation runs:
```python
final_prompt = system_prompt + conversation_mechanics
# system_prompt: from default_prompts.yaml or GEPA-generated
# conversation_mechanics: from conversation_prompts.yaml (always appended)
```

This separation allows:
- Testing different coaching approaches (variable system prompt)
- Maintaining consistent conversation quality (fixed mechanics)
