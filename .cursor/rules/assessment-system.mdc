---
description: Guidelines for the LLM-based assessment system
---

# Assessment System

The assessment system evaluates AI dummy personalities using LLM-based scoring.

## Core Files

- [assessment_system_llm_based.py](mdc:assessment_system_llm_based.py) - Main assessment implementation
- [prompts/assessment_prompts.yaml](mdc:prompts/assessment_prompts.yaml) - Assessment prompt templates
- [models.py](mdc:models.py) - Assessment data models

## Assessment Models

### AssessmentResponse
```python
question: str           # The assessment question
score: int             # Score 1-10
confidence: int        # Confidence 1-10
notes: Optional[str]   # Additional notes
```

### Assessment
```python
dummy_id: str
timestamp: datetime
responses: List[AssessmentResponse]
total_score: float
average_score: float
improvement_areas: List[str]
```

## Assessment Process

1. **Pre-assessment** - Baseline personality evaluation before conversations
2. **Conversations** - Dummy interacts with AI assistant
3. **Post-assessment** - Re-evaluate personality after conversations
4. **Comparison** - Calculate improvement score

## Scoring Conventions

- All scores use **1-4 scale** 
- Higher scores = better social skills / lower anxiety
- `improvement_score` = post_average - pre_average

## Integration with Evolution

When personality evolution is enabled:
- Pre-assessment uses original profile
- Post-assessment uses current (evolved) profile via `get_current_profile_for_assessment()`

## Related Systems

- Uses prompts from [prompts/assessment_prompts.yaml](mdc:prompts/assessment_prompts.yaml)
- Integrates with personality evolution system
- Results tracked in TestSession model
